name: Email Automation Scheduler

on:
  schedule:
    # Runs every 5 minutes
    - cron: '*/5 * * * *'
  workflow_dispatch:

jobs:
  run-email-agent:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    services:
      # --- DEFAULT SERVICE: Ollama ---
      # This service runs the local Ollama instance.
      ollama_service:
        image: ollama/ollama:latest
        ports:
          - 11434:11434
        env:
          OLLAMA_NUM_THREAD: "0"
        options: >-
          --name ollama_service
          --health-cmd="ollama -v"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5

      # --- PLACEHOLDER: AirLLM Service ---
      # If you set up a local AirLLM service in a Docker container,
      # you would define it here. For example:
      #
      # airllm_service:
      #   image: your-airllm-docker-image:latest
      #   ports:
      #     - 5000:5000 # Assuming AirLLM runs on port 5000

    env:
      # --- General Email Secrets ---
      EMAIL_ADDRESS: ${{ secrets.EMAIL_ADDRESS }}
      EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
      IMAP_SERVER: imap.gmail.com
      SMTP_SERVER: smtp.gmail.com
      LANGCHAIN_API_KEY: ${{ secrets.LANGCHAIN_API_KEY }}
      PYTHONUNBUFFERED: 1

      # --- LLM Configuration (Defaults to Ollama) ---
      # Your Python script will use these variables.
      # Set 'USE_AIRLLM' to 'true' in your script to switch.
      OLLAMA_URL: http://localhost:11434/api/generate
      LLAMA_MODEL: mistral:7b-instruct-q4_0

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: |
          pip install --upgrade pip
          pip install requests

      - name: Wait for Ollama Service
        run: |
          echo "Waiting for Ollama service to become healthy..."
          for i in {1..24}; do
            if docker inspect --format='{{json .State.Health.Status}}' ollama_service | grep -q healthy; then
              echo "Ollama service is healthy!"
              break
            else
              echo "Waiting for Ollama... ($i/24)"
              sleep 5
            fi
            if [ $i -eq 24 ]; then
              echo "Ollama service failed to become healthy. Aborting."
              exit 1
            fi
          done

      - name: Pull Mistral 7B Instruct Model
        run: |
          echo "Pulling model ${{ env.LLAMA_MODEL }}..."
          docker exec ollama_service ollama pull ${{ env.LLAMA_MODEL }}

      - name: Warm-up Ollama Model
        run: |
          echo "Warming up model to load it into memory..."
          curl http://localhost:11434/api/generate -d '{
            "model": "${{ env.LLAMA_MODEL }}",
            "prompt": "Hi",
            "stream": false,
            "options": {"num_predict": 1}
          }' --max-time 180

      - name: Run Email Agent
        run: |
          echo "Starting Email Automation..."
          python email_agent.py
