name: Email Automation Scheduler

on:
  schedule:
    # Runs every 15 minutes — adjust if needed
    - cron: '*/15 * * * *'
  workflow_dispatch:

jobs:
  run-email-agent:
    runs-on: ubuntu-latest

    services:
      ollama_service:
        image: ollama/ollama:latest
        ports:
          - 11434:11434
        # ✅ Final: safe syntax with folded scalar to avoid newline bugs
        options: >-
          --name ollama_service
          --health-cmd="ollama -v"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5

    env:
      EMAIL_ADDRESS: ${{ secrets.EMAIL_ADDRESS }}
      EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
      LANGCHAIN_API_KEY: ${{ secrets.LANGCHAIN_API_KEY }}
      LLM_MODEL: mistral:instruct-q4
      OLLAMA_MODEL: mistral:instruct-q4
      IMAP_SERVER: imap.gmail.com
      SMTP_SERVER: smtp.gmail.com

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache Ollama models
        uses: actions/cache@v4
        with:
          path: /root/.ollama/models
          key: ollama-models-${{ env.LLM_MODEL }}

      - name: Install dependencies
        run: |
          pip install requests imapclient

      - name: Pull and preload Ollama model
        run: |
          echo "Waiting for Ollama service to start..."
          for i in {1..10}; do
            if docker exec ollama_service which ollama >/dev/null 2>&1; then
              echo "Ollama service is ready!"
              break
            else
              echo "Waiting for Ollama to initialize... ($i/10)"
              sleep 5
            fi
          done

          echo "Checking for cached model..."
          if ! docker exec ollama_service ollama list | grep -q ${LLM_MODEL}; then
            echo "Model not found locally, pulling..."
            docker exec ollama_service ollama pull ${LLM_MODEL}
          else
            echo "Model already cached, skipping pull."
          fi

          echo "Preloading model..."
          docker exec ollama_service ollama run ${LLM_MODEL} --prompt "Ready" || true

      - name: Run Email Agent
        run: |
          echo "Starting email agent..."
          python email_agent.py
